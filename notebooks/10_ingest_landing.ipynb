{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15552a5d-7d35-427a-8980-282d9d8ef470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "dbutils.widgets.text(\"landing_dir\", \"\")\n",
    "dbutils.widgets.text(\"run_date\", \"\")\n",
    "dbutils.widgets.text(\"db_token\", \"\")\n",
    "\n",
    "landing_dir = dbutils.widgets.get(\"landing_dir\").strip()\n",
    "run_date = dbutils.widgets.get(\"run_date\").strip()\n",
    "db_token = dbutils.widgets.get(\"db_token\").strip()\n",
    "\n",
    "assert landing_dir, \"Param landing_dir vazio\"\n",
    "assert run_date, \"Param run_date vazio\"\n",
    "assert db_token, \"Param db_token vazio\"\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import requests\n",
    "\n",
    "print(f\"[ingest] landing_dir={landing_dir}\")\n",
    "print(f\"[ingest] run_date={run_date}\")\n",
    "\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "host = \"https://\" + workspace_url\n",
    "print(f\"[ingest] host={host}\")\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {db_token}\"}\n",
    "\n",
    "def get_ws_file_bytes(path_users: str) -> bytes:\n",
    "    assert path_users.startswith(\"/Users/\"), f\"Esperava /Users/... mas veio: {path_users}\"\n",
    "    url = f\"{host}/api/2.0/fs/files{path_users}\"\n",
    "    r = requests.get(url, headers=headers)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"Falha ao baixar {path_users}. status={r.status_code} body={r.text[:300]}\")\n",
    "    return r.content\n",
    "\n",
    "events_bytes   = get_ws_file_bytes(f\"{landing_dir}/events.json\")\n",
    "fights_bytes   = get_ws_file_bytes(f\"{landing_dir}/fights.jsonl\")\n",
    "fighters_bytes = get_ws_file_bytes(f\"{landing_dir}/fighters.jsonl\")\n",
    "\n",
    "events_str = events_bytes.decode(\"utf-8\")\n",
    "fights_lines = [l for l in fights_bytes.decode(\"utf-8\").splitlines() if l.strip()]\n",
    "fighters_lines = [l for l in fighters_bytes.decode(\"utf-8\").splitlines() if l.strip()]\n",
    "\n",
    "print(f\"[ingest] downloaded: events_bytes={len(events_bytes)} fights_lines={len(fights_lines)} fighters_lines={len(fighters_lines)}\")\n",
    "\n",
    "events_df = spark.read.json(sc.parallelize([events_str])).withColumn(\"run_date\", F.lit(run_date))\n",
    "fights_df = spark.read.json(sc.parallelize(fights_lines)).withColumn(\"run_date\", F.lit(run_date))\n",
    "fighters_df = spark.read.json(sc.parallelize(fighters_lines)).withColumn(\"run_date\", F.lit(run_date))\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ufc\")\n",
    "spark.sql(\"USE ufc\")\n",
    "\n",
    "events_df.write.mode(\"append\").format(\"delta\").partitionBy(\"run_date\").saveAsTable(\"ufc.bronze_ufc_events\")\n",
    "fights_df.write.mode(\"append\").format(\"delta\").partitionBy(\"run_date\").saveAsTable(\"ufc.bronze_ufc_fights\")\n",
    "fighters_df.write.mode(\"append\").format(\"delta\").partitionBy(\"run_date\").saveAsTable(\"ufc.bronze_ufc_fighters\")\n",
    "\n",
    "print(\"[ingest] âœ… bronze tables updated\")\n",
    "display(fights_df.limit(10))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "10_ingest_landing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
