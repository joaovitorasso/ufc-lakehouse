name: UFC Lakehouse (extract -> upload -> run)

on:
  workflow_dispatch:
  schedule:
    # 07:00 no Brasil (America/Sao_Paulo) ~ 10:00 UTC
    - cron: "0 10 * * *"

jobs:
  pipeline:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Debug paths
        run: |
          pwd
          ls -la
          echo "GITHUB_WORKSPACE=$GITHUB_WORKSPACE"

      - name: Extract landing ZIP
        env:
          MAX_EVENTS: ${{ secrets.MAX_EVENTS }}
        run: |
          export RUN_DATE=$(date -u +%F)
          python -m tools.extract_landing
          ls -lah out/dt=${RUN_DATE}

      - name: Upload ZIP to Workspace Files (WSFS)
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        run: |
          RUN_DATE=$(date -u +%F)
          LOCAL_ZIP=$(ls -1 out/*.zip | head -n 1)

          # caminho onde o arquivo vai ficar no Workspace Files
          WS_DIR="/Workspace/Users/${{ secrets.DATABRICKS_USER }}/ufc-lakehouse/landing/dt=${RUN_DATE}"
          WS_FILE="${WS_DIR}/ufc_landing.zip"

          # 1) cria diret√≥rio (WSFS)
          curl -sS -X PUT "https://${DATABRICKS_HOST}/api/2.0/fs/directories${WS_DIR}" \
            -H "Authorization: Bearer ${DATABRICKS_TOKEN}"

          # 2) upload do arquivo (WSFS)
          curl -sS -X PUT "https://${DATABRICKS_HOST}/api/2.0/fs/files${WS_FILE}?overwrite=true" \
            -H "Authorization: Bearer ${DATABRICKS_TOKEN}" \
            -H "Content-Type: application/octet-stream" \
            --data-binary "@${LOCAL_ZIP}"

      - name: Trigger Databricks job
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          DATABRICKS_JOB_ID: ${{ secrets.DATABRICKS_JOB_ID }}
        run: |
          RUN_DATE=$(date -u +%F)
          LANDING_ZIP="dbfs:/tmp/ufc/landing/dt=${RUN_DATE}/ufc_landing.zip"

          curl -sS --location --request POST "https://${DATABRICKS_HOST}/api/2.2/jobs/run-now" \
            --header "Authorization: Bearer ${DATABRICKS_TOKEN}" \
            --header "Content-Type: application/json" \
            --data "{
              \"job_id\": ${DATABRICKS_JOB_ID},
              \"notebook_params\": {
                \"landing_zip\": \"${LANDING_ZIP}\",
                \"run_date\": \"${RUN_DATE}\"
              }
            }"
