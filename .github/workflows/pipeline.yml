name: UFC Lakehouse (extract -> upload -> run)

on:
  workflow_dispatch:
  schedule:
    # 07:00 no Brasil (America/Sao_Paulo) ~ 10:00 UTC
    - cron: "0 10 * * *"

jobs:
  pipeline:
    runs-on: ubuntu-latest

    env:
      MAX_FIGHTERS: "50"
      FIGHTER_LOG_EVERY: "10"
      RATE_LIMIT_SLEEP: "0.0"
      RATE_LIMIT_SLEEP_FIGHT: "0.0"

      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      DATABRICKS_USER: ${{ secrets.DATABRICKS_USER }}
      DATABRICKS_JOB_ID: ${{ secrets.DATABRICKS_JOB_ID }}

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ✅ RUN_DATE único para o job inteiro (vale para todos os steps)
      - name: Set RUN_DATE
        run: echo "RUN_DATE=$(date -u +%F)" >> $GITHUB_ENV

      - name: Debug paths
        run: |
          echo "RUN_DATE=${RUN_DATE}"
          pwd
          ls -la
          echo "GITHUB_WORKSPACE=$GITHUB_WORKSPACE"

      - name: Extract landing
        run: |
          echo "RUN_DATE=${RUN_DATE}"
          python -m tools.extract_landing
          ls -lah "out/dt=${RUN_DATE}"

      - name: Upload landing files to Workspace Files (WSFS)
        run: |
          set -euo pipefail

          LOCAL_DIR="out/dt=${RUN_DATE}"
          echo "LOCAL_DIR=${LOCAL_DIR}"
          ls -lah "${LOCAL_DIR}"

          WS_DIR="/Users/${DATABRICKS_USER}/ufc-lakehouse/landing/dt=${RUN_DATE}"
          echo "WS_DIR=${WS_DIR}"

          # URL-encode helper
          enc() {
            python -c "import urllib.parse,sys; print(urllib.parse.quote(sys.argv[1], safe=''))" "$1"
          }

          WS_DIR_ENC="$(enc "${WS_DIR}")"

          echo "Creating dir..."
          curl -sS -X PUT "https://${DATABRICKS_HOST}/api/2.0/fs/directories?path=${WS_DIR_ENC}" \
            -H "Authorization: Bearer ${DATABRICKS_TOKEN}"

          for f in events.json fights.jsonl fighters.jsonl; do
            LOCAL_FILE="${LOCAL_DIR}/${f}"
            test -f "${LOCAL_FILE}"

            WS_FILE="${WS_DIR}/${f}"
            WS_FILE_ENC="$(enc "${WS_FILE}")"

            echo "Uploading ${LOCAL_FILE} -> ${WS_FILE}"
            curl -sS -X PUT "https://${DATABRICKS_HOST}/api/2.0/fs/files?path=${WS_FILE_ENC}&overwrite=true" \
              -H "Authorization: Bearer ${DATABRICKS_TOKEN}" \
              -H "Content-Type: application/octet-stream" \
              --data-binary "@${LOCAL_FILE}"
          done

          echo "✅ Upload finished"

      - name: Trigger Databricks job
        run: |
          set -euo pipefail

          LANDING_DIR="/Users/${DATABRICKS_USER}/ufc-lakehouse/landing/dt=${RUN_DATE}"

          cat > payload.json <<EOF
          {
            "job_id": ${DATABRICKS_JOB_ID},
            "notebook_params": {
              "landing_dir": "${LANDING_DIR}",
              "run_date": "${RUN_DATE}",
              "db_token": "${DATABRICKS_TOKEN}"
            }
          }
          EOF

          echo "Payload (token masked):"
          sed 's/"db_token": ".*"/"db_token": "***"/' payload.json

          echo "Calling run-now..."
          curl -sS --location --request POST "https://${DATABRICKS_HOST}/api/2.2/jobs/run-now" \
            --header "Authorization: Bearer ${DATABRICKS_TOKEN}" \
            --header "Content-Type: application/json" \
            --data @payload.json
